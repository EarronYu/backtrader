{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 导入必要的库\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "def process_file(args):\n",
    "    \"\"\"\n",
    "    处理单个文件\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path, folder_path = args\n",
    "        full_path = os.path.join(folder_path, file_path)\n",
    "        \n",
    "        if not os.path.exists(full_path):\n",
    "            return f\"文件不存在: {full_path}\"\n",
    "            \n",
    "        # 快速检查表头\n",
    "        try:\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                header = f.readline().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(full_path, 'r', encoding='gbk') as f:\n",
    "                header = f.readline().strip()\n",
    "        \n",
    "        # 如果不需要更新，直接跳过\n",
    "        if 'candle_begin_time' not in header:\n",
    "            return f\"跳过: {full_path}\"\n",
    "            \n",
    "        # 分块读取和处理文件\n",
    "        temp_path = full_path + '.tmp'\n",
    "        chunk_size = 1024 * 1024  # 1MB chunks\n",
    "        \n",
    "        with open(full_path, 'rb') as f_in, open(temp_path, 'wb') as f_out:\n",
    "            # 写入新表头\n",
    "            new_header = header.replace('candle_begin_time', 'datetime') + '\\n'\n",
    "            f_out.write(new_header.encode('utf-8'))\n",
    "            \n",
    "            # 跳过原文件的表头行\n",
    "            f_in.readline()\n",
    "            \n",
    "            # 分块复制剩余内容\n",
    "            while True:\n",
    "                chunk = f_in.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f_out.write(chunk)\n",
    "                \n",
    "        # 替换原文件\n",
    "        os.replace(temp_path, full_path)\n",
    "        \n",
    "        return f\"已更新: {full_path}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"错误 {full_path if 'full_path' in locals() else '未知文件'}: {str(e)}\"\n",
    "    finally:\n",
    "        # 强制清理内存\n",
    "        gc.collect()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"\n",
    "    获取当前进程的内存使用情况\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # 转换为MB\n",
    "\n",
    "def update_csv_headers_threaded(base_path):\n",
    "    \"\"\"\n",
    "    使用多线程处理文件\n",
    "    \"\"\"\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] 开始收集文件列表...\")\n",
    "    \n",
    "    # 获取所有文件路径\n",
    "    all_files = []\n",
    "    for date_folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, date_folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            try:\n",
    "                csv_files = [(f, folder_path) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "                all_files.extend(csv_files)\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] 已添加文件夹 {date_folder}: {len(csv_files)} 个文件\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] 读取文件夹 {date_folder} 时出错: {e}\")\n",
    "    \n",
    "    total_files = len(all_files)\n",
    "    print(f\"\\n[{time.strftime('%H:%M:%S')}] 总文件数: {total_files}\")\n",
    "    \n",
    "    # 创建计数器和内存监控\n",
    "    processed_count = 0\n",
    "    lock = threading.Lock()\n",
    "    max_memory = 0\n",
    "    \n",
    "    def update_progress(future):\n",
    "        nonlocal processed_count, max_memory\n",
    "        with lock:\n",
    "            processed_count += 1\n",
    "            current_memory = get_memory_usage()\n",
    "            max_memory = max(max_memory, current_memory)\n",
    "            \n",
    "            if processed_count % 10 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                speed = processed_count / elapsed_time\n",
    "                remaining = (total_files - processed_count) / speed if speed > 0 else 0\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] 进度: {processed_count}/{total_files} \"\n",
    "                      f\"({processed_count/total_files*100:.2f}%) \"\n",
    "                      f\"速度: {speed:.2f} 文件/秒 \"\n",
    "                      f\"预计剩余时间: {remaining/60:.2f} 分钟 \"\n",
    "                      f\"内存使用: {current_memory:.1f}MB\")\n",
    "                \n",
    "                # 如果内存使用过高，强制进行垃圾回收\n",
    "                if current_memory > 1000:  # 超过1GB时\n",
    "                    gc.collect()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 使用线程池处理文件\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:  # 减少线程数\n",
    "        # 分批提交任务\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(all_files), batch_size):\n",
    "            batch = all_files[i:i+batch_size]\n",
    "            \n",
    "            # 提交批次任务\n",
    "            future_to_file = {executor.submit(process_file, file_info): file_info \n",
    "                            for file_info in batch}\n",
    "            \n",
    "            # 添加回调\n",
    "            for future in future_to_file:\n",
    "                future.add_done_callback(update_progress)\n",
    "            \n",
    "            # 获取结果\n",
    "            for future in future_to_file:\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result.startswith((\"已更新\", \"错误\")):\n",
    "                        print(f\"[{time.strftime('%H:%M:%S')}] {result}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] 处理出错: {e}\")\n",
    "            \n",
    "            # 批次处理完成后强制清理内存\n",
    "            gc.collect()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n[{time.strftime('%H:%M:%S')}] 处理完成！\")\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] 总处理文件数: {processed_count}\")\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] 总用时: {total_time:.2f}秒\")\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] 平均速度: {processed_count/total_time:.2f} 文件/秒\")\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] 最大内存使用: {max_memory:.1f}MB\")\n",
    "\n",
    "# 执行处理\n",
    "base_path = r'\\\\znas\\Main\\spot'\n",
    "update_csv_headers_threaded(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 定义配置\n",
    "TIME_INTERVAL = '1m'  # 添加时间间隔配置\n",
    "COLORS = {\n",
    "    0: 'red',     # 文件不存在\n",
    "    1: 'green',   # 数据完整\n",
    "    2: 'yellow',  # 有时间断层\n",
    "    3: 'orange',  # 数据点不足\n",
    "    4: 'purple'   # 处理错误\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spot_data_integrity(base_path, symbols, start_date, end_date):\n",
    "    \"\"\"\n",
    "    检查现货数据完整性\n",
    "    \n",
    "    参数:\n",
    "    base_path (str): 数据根目录\n",
    "    symbols (list): 交易对列表\n",
    "    start_date (str): 起始日期 (YYYY-MM-DD)\n",
    "    end_date (str): 结束日期 (YYYY-MM-DD)\n",
    "    \n",
    "    返回:\n",
    "    DataFrame: 数据完整性检查结果\n",
    "    \"\"\"\n",
    "    # 生成日期范围\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    date_list = [str(date.date()) for date in date_range]\n",
    "    \n",
    "    # 创建结果DataFrame\n",
    "    results = pd.DataFrame(index=date_list, columns=symbols)\n",
    "    results = results.fillna(0)  # 默认填充0，表示数据不存在\n",
    "    \n",
    "    # 遍历每个交易对和日期\n",
    "    for symbol in symbols:\n",
    "        for date in date_list:\n",
    "            file_path = os.path.join(\n",
    "                base_path, \n",
    "                'data', \n",
    "                'spot',\n",
    "                date,\n",
    "                f\"{date}_{symbol.replace('/', '')}_{TIME_INTERVAL}.csv\"  # 使用定义的时间间隔\n",
    "            )\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"缺失文件: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # 读取数据\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # 基本数据检查\n",
    "                if df.empty:\n",
    "                    print(f\"空文件: {file_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                # 检查必要列\n",
    "                required_cols = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "                if not all(col in df.columns for col in required_cols):\n",
    "                    print(f\"列缺失: {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # 转换时间列\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "                \n",
    "                # 检查时间间隔（对于1分钟数据）\n",
    "                time_diffs = df['datetime'].diff()\n",
    "                expected_diff = pd.Timedelta(minutes=1)\n",
    "                \n",
    "                # 检查是否有时间断层\n",
    "                gaps = time_diffs[time_diffs > expected_diff]\n",
    "                if not gaps.empty:\n",
    "                    print(f\"发现时间断层 {file_path}:\")\n",
    "                    for idx in gaps.index:\n",
    "                        gap_start = df['datetime'][idx-1]\n",
    "                        gap_end = df['datetime'][idx]\n",
    "                        print(f\"  断层: {gap_start} 到 {gap_end}\")\n",
    "                    results.loc[date, symbol] = 2  # 2表示有时间断层\n",
    "                else:\n",
    "                    # 检查数据点数量（一天应该有1440个1分钟数据）\n",
    "                    expected_points = 1440\n",
    "                    if len(df) < expected_points:\n",
    "                        print(f\"数据点不足: {file_path}, 实际: {len(df)}, 应有: {expected_points}\")\n",
    "                        results.loc[date, symbol] = 3  # 3表示数据点不足\n",
    "                    else:\n",
    "                        results.loc[date, symbol] = 1  # 1表示数据完整\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"处理文件出错 {file_path}: {str(e)}\")\n",
    "                results.loc[date, symbol] = 4  # 4表示处理错误\n",
    "    \n",
    "    # 保存检查结果\n",
    "    result_path = os.path.join(base_path, 'data_integrity_check.xlsx')\n",
    "    \n",
    "    # 创建一个样式化的Excel文件\n",
    "    def color_cells(val):\n",
    "        colors = {\n",
    "            0: 'background-color: red',     # 文件不存在\n",
    "            1: 'background-color: green',   # 数据完整\n",
    "            2: 'background-color: yellow',  # 有时间断层\n",
    "            3: 'background-color: orange',  # 数据点不足\n",
    "            4: 'background-color: purple'   # 处理错误\n",
    "        }\n",
    "        return colors.get(val, '')\n",
    "    \n",
    "    # 保存带样式的结果\n",
    "    styled_results = results.style.applymap(color_cells)\n",
    "    styled_results.to_excel(result_path)\n",
    "    \n",
    "    print(f\"\\n检查结果已保存至: {result_path}\")\n",
    "    print(\"\\n状态说明:\")\n",
    "    print(\"0 (红色) - 文件不存在\")\n",
    "    print(\"1 (绿色) - 数据完整\")\n",
    "    print(\"2 (黄色) - 有时间断层\")\n",
    "    print(\"3 (橙色) - 数据点不足\")\n",
    "    print(\"4 (紫色) - 处理错误\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-08\\2024-01-08_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-09\\2024-01-09_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-10\\2024-01-10_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-11\\2024-01-11_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-12\\2024-01-12_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-13\\2024-01-13_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-14\\2024-01-14_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-15\\2024-01-15_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-16\\2024-01-16_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-17\\2024-01-17_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-18\\2024-01-18_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-19\\2024-01-19_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-20\\2024-01-20_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-21\\2024-01-21_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-22\\2024-01-22_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-23\\2024-01-23_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-24\\2024-01-24_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-25\\2024-01-25_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-26\\2024-01-26_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-27\\2024-01-27_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-28\\2024-01-28_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-29\\2024-01-29_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-30\\2024-01-30_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-31\\2024-01-31_BTCUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-01\\2024-01-01_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-02\\2024-01-02_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-03\\2024-01-03_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-04\\2024-01-04_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-05\\2024-01-05_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-06\\2024-01-06_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-07\\2024-01-07_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-08\\2024-01-08_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-09\\2024-01-09_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-10\\2024-01-10_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-11\\2024-01-11_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-12\\2024-01-12_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-13\\2024-01-13_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-14\\2024-01-14_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-15\\2024-01-15_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-16\\2024-01-16_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-17\\2024-01-17_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-18\\2024-01-18_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-19\\2024-01-19_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-20\\2024-01-20_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-21\\2024-01-21_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-22\\2024-01-22_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-23\\2024-01-23_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-24\\2024-01-24_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-25\\2024-01-25_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-26\\2024-01-26_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-27\\2024-01-27_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-28\\2024-01-28_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-29\\2024-01-29_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-30\\2024-01-30_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-31\\2024-01-31_ETHUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-01\\2024-01-01_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-02\\2024-01-02_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-03\\2024-01-03_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-04\\2024-01-04_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-05\\2024-01-05_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-06\\2024-01-06_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-07\\2024-01-07_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-08\\2024-01-08_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-09\\2024-01-09_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-10\\2024-01-10_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-11\\2024-01-11_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-12\\2024-01-12_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-13\\2024-01-13_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-14\\2024-01-14_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-15\\2024-01-15_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-16\\2024-01-16_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-17\\2024-01-17_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-18\\2024-01-18_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-19\\2024-01-19_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-20\\2024-01-20_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-21\\2024-01-21_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-22\\2024-01-22_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-23\\2024-01-23_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-24\\2024-01-24_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-25\\2024-01-25_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-26\\2024-01-26_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-27\\2024-01-27_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-28\\2024-01-28_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-29\\2024-01-29_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-30\\2024-01-30_BNBUSDT_1m.csv\n",
      "缺失文件: D:/CryptoData\\data\\spot\\2024-01-31\\2024-01-31_BNBUSDT_1m.csv\n",
      "\n",
      "检查结果已保存至: D:/CryptoData\\data_integrity_check.xlsx\n",
      "\n",
      "状态说明:\n",
      "0 (红色) - 文件不存在\n",
      "1 (绿色) - 数据完整\n",
      "2 (黄色) - 有时间断层\n",
      "3 (橙色) - 数据点不足\n",
      "4 (紫色) - 处理错误\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\x7498\\AppData\\Local\\Temp\\ipykernel_60048\\2849669004.py:20: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  results = results.fillna(0)  # 默认填充0，表示数据不存在\n",
      "C:\\Users\\x7498\\AppData\\Local\\Temp\\ipykernel_60048\\2849669004.py:96: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styled_results = results.style.applymap(color_cells)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 运行示例\n",
    "if __name__ == '__main__':\n",
    "    # 配置参数\n",
    "    params = {\n",
    "        'base_path': 'D:/CryptoData',  # 修改为正确的路径\n",
    "        'begin_date': '2024-01-01',\n",
    "        'end_date': '2024-01-31'\n",
    "    }\n",
    "    \n",
    "    target_symbols = [\n",
    "        'BTC/USDT',\n",
    "        'ETH/USDT',\n",
    "        'BNB/USDT'\n",
    "    ]\n",
    "    \n",
    "    # 运行检查\n",
    "    results = check_spot_data_integrity(\n",
    "        base_path=params['base_path'],\n",
    "        symbols=target_symbols,\n",
    "        start_date=params['begin_date'],\n",
    "        end_date=params['end_date']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backtrader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
